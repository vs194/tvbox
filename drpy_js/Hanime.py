# -*- coding: utf-8 -*-
# by @å—·å‘œ & Perplexity (Updated 2025-12-02) - å¤šåˆ†è¾¨ç‡é€‰æ‹©ç‰ˆ
import json
import sys
import threading
import requests
import re
import time
import random
import html as html_parser
from urllib.parse import quote, unquote

sys.path.append('..')
from base.spider import Spider

class Spider(Spider):

    def init(self, extend=""):
        # å¼ºåˆ¶ä½¿ç”¨ä½ æä¾›çš„åä»£ï¼Œä¿è¯èƒ½æ‹¿åˆ°é¡µé¢æ•°æ®
        self.host = "https://down.nigx.cn/hanime1.me"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': f'{self.host}/',
            'Origin': self.host
        }

    def getName(self):
        return "Hanime1"

    def isVideoFormat(self, url):
        return any(ext in (url or '') for ext in ['.m3u8', '.mp4', '.ts'])

    def manualVideoCheck(self):
        return False

    def destroy(self):
        pass

    # =================== é¦–é¡µä¸åˆ†ç±» ===================

    def homeContent(self, filter):
        classes = [
            {'type_name': 'æœ€æ–°ä¸Šå¸‚', 'type_id': 'latest'},
            {'type_name': 'è£ç•ª', 'type_id': 'è£ç•ª'},
            {'type_name': 'æ³¡éºµç•ª', 'type_id': 'æ³¡éºµç•ª'},
            {'type_name': 'Motion Anime', 'type_id': 'Motion Anime'},
            {'type_name': '3Då‹•ç•«', 'type_id': '3Då‹•ç•«'},
            {'type_name': 'åŒäººä½œå“', 'type_id': 'åŒäººä½œå“'},
            {'type_name': 'MMD', 'type_id': 'MMD'},
            {'type_name': 'Cosplay', 'type_id': 'Cosplay'},
            {'type_name': 'æœ¬æ—¥æ’è¡Œ', 'type_id': 'daily_rank'},
            {'type_name': 'æœ¬é€±æ’è¡Œ', 'type_id': 'weekly_rank'},
            {'type_name': 'æœ¬æœˆæ’è¡Œ', 'type_id': 'monthly_rank'}
        ]

        sort_filters = [
            {"n": "æœ€æ–°", "v": "æœ€æ–°ä¸Šå¸‚"},
            {"n": "ç†±é–€", "v": "äººæ°£çˆ†æ£š"}
        ]

        filters = {}
        for item in classes:
            filters[item['type_id']] = [
                {"key": "sort", "name": "æ’åº", "value": sort_filters}
            ]

        return {'class': classes, 'filters': filters}

    def homeVideoContent(self):
        try:
            url = f"{self.host}/search?sort=æœ€æ–°ä¸Šå¸‚"
            content = self.fetch(url, headers=self.getheaders()).text
            vods = self.parse_vod_list(content)
            return {'list': vods}
        except Exception:
            return {'list': []}

    def categoryContent(self, tid, pg, filter, extend):
        page = int(pg)
        sort = extend.get('sort', 'æœ€æ–°ä¸Šå¸‚')

        if tid == 'latest':
            url = f"{self.host}/search?sort={sort}&page={page}"
        elif tid == 'daily_rank':
            url = f"{self.host}/search?sort=æœ¬æ—¥æ’è¡Œ&page={page}"
        elif tid == 'weekly_rank':
            url = f"{self.host}/search?sort=æœ¬é€±æ’è¡Œ&page={page}"
        elif tid == 'monthly_rank':
            url = f"{self.host}/search?sort=æœ¬æœˆæ’è¡Œ&page={page}"
        else:
            url = f"{self.host}/search?query={quote(tid)}&sort={sort}&page={page}"

        try:
            content = self.fetch(url, headers=self.getheaders()).text
            vods = self.parse_vod_list(content)
            return {
                'list': vods,
                'page': page,
                'pagecount': page + 1 if len(vods) > 0 else page,
                'limit': 30,
                'total': 9999
            }
        except Exception:
            return {'list': []}

    # =================== è¯¦æƒ…é¡µ - å¤šåˆ†è¾¨ç‡ç‰ˆæœ¬ ===================

    def detailContent(self, ids):
        vid = ids[0]
        url = f"{self.host}/watch?v={vid}"

        try:
            html = self.fetch(url, headers=self.getheaders()).text

            # æ ‡é¢˜ã€å°é¢ã€ç®€ä»‹
            title_match = re.search(r'<meta property="og:title" content="(.*?)"', html)
            title = title_match.group(1) if title_match else vid

            pic_match = re.search(r'<meta property="og:image" content="(.*?)"', html)
            pic = pic_match.group(1) if pic_match else ""

            desc_match = re.search(r'<meta property="og:description" content="(.*?)"', html)
            desc = desc_match.group(1) if desc_match else ""

            # æ ‡ç­¾åŠŸèƒ½
            vod_tag_list = []
            rich_tags = []
            
            tag_matches = re.findall(
                r'<div class="single-video-tag"[^>]*>\s*<a[^>]+>(.*?)</a>',
                html, re.S
            )
            
            seen_tags = set()
            for inner_html in tag_matches:
                name = re.sub(r'<[^>]+>', '', inner_html).strip()
                name = re.sub(r'\s*\d+$', '', name).strip()
                name = html_parser.unescape(name).replace('&nbsp;', '')
                
                if name and name not in seen_tags:
                    seen_tags.add(name)
                    vod_tag_list.append(name)
                    target = json.dumps({'id': name, 'name': name}, ensure_ascii=False)
                    rich_tags.append(f'[a=cr:{target}/]{name}[/a]')

            vod_tag_str = ",".join(vod_tag_list)
            vod_content = f"{desc}\n\nğŸ·ï¸ æ ‡ç­¾: {' '.join(rich_tags)}" if rich_tags else desc

            # ========== å¤šåˆ†è¾¨ç‡è§£æ ==========
            sources = re.findall(r'<source[^>]+src="([^"]+)"', html)
            if not sources:
                sources = re.findall(r'src="([^"]+\.mp4[^"]*)"', html)

            decoded_sources = []
            if sources:
                decoded_sources = [html_parser.unescape(s).replace('&amp;', '&') for s in sources]

            # æ™ºèƒ½åˆ†è¾¨ç‡åˆ†ç±»ï¼Œæ”¯æŒ4Kã€2Kã€1080pã€720pã€480pç­‰
            quality_map = {}
            for s in decoded_sources:
                if '4k' in s.lower() or '2160' in s:
                    quality_map.setdefault('4K', []).append(s)
                elif '2k' in s.lower() or '1440' in s:
                    quality_map.setdefault('2K', []).append(s)
                elif '1080' in s:
                    quality_map.setdefault('1080p', []).append(s)
                elif '720' in s:
                    quality_map.setdefault('720p', []).append(s)
                elif '480' in s:
                    quality_map.setdefault('480p', []).append(s)
                else:
                    quality_map.setdefault('æ ‡æ¸…', []).append(s)

            # ä¼˜å…ˆçº§æ’åºï¼š4K > 2K > 1080p > 720p > 480p > æ ‡æ¸…
            quality_order = ['4K', '2K', '1080p', '720p', '480p', 'æ ‡æ¸…']
            play_parts = []
            
            for q in quality_order:
                if q in quality_map and quality_map[q]:
                    best_url = quality_map[q][0]  # å–ç¬¬ä¸€ä¸ªæœ€ä½³é“¾æ¥
                    play_url_with_dm = f"{vid}_dm_{best_url}"
                    play_parts.append(f"{q}${play_url_with_dm}")

            # å…œåº•m3u8
            if not play_parts:
                m3u8_match = re.search(r'source\s*=\s*[\'"](https?://[^\'"]+\.m3u8[^\'"]*)[\'"]', html)
                if m3u8_match:
                    best_url = m3u8_match.group(1).replace('&amp;', '&')
                    play_url_with_dm = f"{vid}_dm_{best_url}"
                    play_parts.append(f"è‡ªåŠ¨${play_url_with_dm}")

            if not play_parts:
                play_parts.append(f"ç½‘é¡µæ’­æ”¾${url}")

            # æ ‡å‡†æ’­æ”¾æ ¼å¼ï¼šçº¿è·¯$æ¸…æ™°åº¦1$åœ°å€1#æ¸…æ™°åº¦2$åœ°å€2#...
            line_name = "ä¹¦ç”Ÿç©å‰£â±Â·*â‚ï¼‡"
            vod_play_url = f"{line_name}$" + "#".join(play_parts)

            vod = {
                "vod_id": vid,
                "vod_name": title,
                "vod_pic": pic,
                "type_name": "Hanime",
                "vod_year": "",
                "vod_area": "Japan",
                "vod_remarks": "Hanime",
                "vod_actor": "",
                "vod_director": "",
                "vod_content": vod_content,
                "vod_tag": vod_tag_str,
                "vod_play_from": line_name,
                "vod_play_url": vod_play_url
            }

            return {'list': [vod]}

        except Exception:
            return {'list': []}

    # =================== æœç´¢ ===================

    def searchContent(self, key, quick, pg="1"):
        url = f"{self.host}/search?query={quote(key)}&page={pg}"
        try:
            content = self.fetch(url, headers=self.getheaders()).text
            vods = self.parse_vod_list(content)
            return {'list': vods, 'page': pg}
        except Exception:
            return {'list': []}

    # =================== æ’­æ”¾ ===================

    def playerContent(self, flag, id, vipFlags):
        header = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://hanime1.me/',
            'Origin': 'https://hanime1.me'
        }

        if '_dm_' in id:
            vid, url = id.split('_dm_', 1)
            threading.Thread(target=self._preload_danmaku, args=(vid, url)).start()
        else:
            url = id

        if '.mp4' in url or '.m3u8' in url:
            return {'parse': 0, 'playUrl': '', 'url': url, 'header': header}

        return {'parse': 1, 'playUrl': '', 'url': url, 'header': header}

    # =================== åˆ—è¡¨è§£æï¼šä¿æŒåŸæ · ===================

    def parse_vod_list(self, html):
        vods = []
        seen = set()

        parts = re.split(r'class="[^"]*search-doujin-videos[^"]*"', html)

        for i in range(1, len(parts)):
            block = parts[i][:4000]
            try:
                url_match = re.search(r'<a[^>]+href="([^"]+)"', block)
                if not url_match: continue
                url = url_match.group(1)

                id_match = re.search(r'v=(\d+)', url)
                if not id_match: continue
                vid = id_match.group(1)

                if vid in seen: continue
                seen.add(vid)

                title = vid
                title_match = re.search(r'class="[^"]*card-mobile-title[^"]*"[^>]*>(.*?)</div>', block)
                if title_match:
                    title = title_match.group(1).strip()

                pic = ""
                img_matches = re.findall(r'<img[^>]+src="([^"]+)"', block)
                found_thumb = False
                for img_src in img_matches:
                    if 'thumbnail' in img_src:
                        pic = img_src
                        found_thumb = True
                        break
                if not found_thumb and len(img_matches) > 1:
                    pic = img_matches[1]
                elif not found_thumb and len(img_matches) > 0:
                    pic = img_matches[0]

                remarks = ""
                dur_match = re.search(r'class="[^"]*card-mobile-duration[^"]*"[^>]*>(.*?)</div>', block)
                if dur_match:
                    remarks = dur_match.group(1).strip()

                vods.append({
                    "vod_id": vid,
                    "vod_name": title,
                    "vod_pic": pic,
                    "vod_remarks": remarks
                })
            except Exception:
                pass

        return vods

    # =================== å¼¹å¹•æ”¯æŒ ===================

    def localProxy(self, param):
        try:
            xtype = param.get('type', '')
            if xtype == 'hlxdm':
                vid = param.get('path', '')
                times = int(param.get('times', 0))
                comments = self._fetch_comments(vid)
                return self._generate_danmaku_xml(comments, times)
            return [404, 'text/plain', b'']
        except Exception:
            return [500, 'text/plain', b'']

    def _fetch_comments(self, vid):
        """ä¸“é—¨è§£æ Hanime1 çš„ HTML è¯„è®ºç»“æ„"""
        comments = []
        try:
            url = f"{self.host}/loadComment?id={vid}&type=video&content=comment-tablink"
            res = requests.get(url, headers=self.getheaders(), timeout=5)
            
            # è§£æ JSONï¼Œè·å– comments HTML å­—ç¬¦ä¸²
            data = res.json()
            comments_html = data.get('comments', '')
            
            if not comments_html:
                return ["æ¬¢è¿è§‚çœ‹", "Hanime1"]
            
            # æ­£åˆ™æå–è¯„è®ºå†…å®¹ï¼šclass="comment-index-text" é‡Œçš„è¯„è®ºæ­£æ–‡
            # åŒ¹é…ç¬¬äºŒä¸ª comment-index-textï¼ˆç”¨æˆ·åæ˜¯ç¬¬ä¸€ä¸ªï¼Œè¯„è®ºæ˜¯ç¬¬äºŒä¸ªï¼‰
            comment_blocks = re.findall(
                r'<div[^>]*class="comment-index-text"[^>]*>(?:[^<]|<[^>]*>)*?</div>\s*<div[^>]*class="comment-index-text"[^>]*>(.*?)</div>',
                comments_html, re.S
            )
            
            # æå–è¯„è®ºæ–‡æœ¬ï¼Œå»HTMLæ ‡ç­¾
            for block in comment_blocks:
                text = re.sub(r'<[^>]+>', '', block).strip()
                # è¿‡æ»¤çº¯UIæ–‡æœ¬ï¼Œä¿ç•™çœŸå®è¯„è®º
                if len(text) > 2 and len(text) < 100 and not any(x in text for x in ['åŠ è½½ä¸­', 'æŸ¥çœ‹', 'å›å¤', 'ç™»å½•', 'å‘è¡¨']):
                    comments.append(text)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            seen = set()
            clean_comments = []
            for c in comments:
                if c not in seen and len(clean_comments) < 60:
                    seen.add(c)
                    clean_comments.append(c)
            
            return clean_comments if clean_comments else ["æ¬¢è¿è§‚çœ‹", "ç²¾å½©å†…å®¹"]
            
        except Exception:
            return ["æ¬¢è¿è§‚çœ‹", "Hanime1"]

    def _generate_danmaku_xml(self, comments, duration):
        if duration <= 0:
            duration = 600  # é»˜è®¤10åˆ†é’Ÿ
            
        xml = ['<?xml version="1.0" encoding="UTF-8"?>', '<i>']
        xml.append('<d p="0,5,25,16711680,0">å¼¹å¹•åŠ è½½æˆåŠŸ</d>')
        
        # å¦‚æœæ²¡è¯„è®ºï¼Œæ·»åŠ å ä½å¼¹å¹•
        if not comments:
            xml.extend([
                '<d p="5,1,25,16777215,0">æš‚æ— è¯„è®ºï¼Œæ¬¢è¿è¡¥å……~</d>',
                '<d p="15,1,25,16777215,0">Hanime1 é«˜æ¸…æ— ç </d>'
            ])
        else:
            # æŒ‰è§†é¢‘æ—¶é•¿å‡åŒ€åˆ†å¸ƒè¯„è®º
            for i, comment in enumerate(comments):
                progress = i / len(comments)
                base_time = progress * duration
                t = round(max(1, min(base_time + random.uniform(-5, 5), duration - 1)), 1)
                
                color = 16777215  # ç™½è‰²
                if random.random() < 0.15:
                    color = random.randint(0x666666, 0xFFFFFF)
                
                safe_text = html_parser.escape(comment)
                xml.append(f'<d p="{t},1,25,{color},0">{safe_text}</d>')
        
        xml.append('</i>')
        return [200, 'text/xml', '\n'.join(xml)]

    def _preload_danmaku(self, vid, url):
        try:
            time.sleep(1)
            dm_url = f"{self.getProxyUrl()}&path={vid}&times=600&type=hlxdm"
            requests.get(f"http://127.0.0.1:9978/action?do=refresh&type=danmaku&path={quote(dm_url)}", timeout=2)
        except:
            pass

    # =================== é€šç”¨å·¥å…· ===================

    def getheaders(self, param=None):
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': f'{self.host}/',
            'Origin': self.host
        }
        return headers
